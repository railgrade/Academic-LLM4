# Open Source LLMs

Table of Contents

- [Pretrained Model](#pretrained-model)
- [Multitask Supervised Finetuned Model](#multitask-supervised-finetuned-model)
- [Instruction Finetuned Model](#instruction-finetuned-model)
  - [English](#english)
  - [Chinese](#chinese)
  - [Multilingual](#multilingual)
- [Human Feedback Finetuned Model](#human-feedback-finetuned-models)
- [Domain Finetuned Model](#domain-finetuned-model)
- [Open Source Projects](#open-source-projects)
  - [reproduce/framework](#reproduceframework)
  - [accelerate](#accelerate)
  - [evaluation](#evaluation)
  - [deployment/demo](#deploymentdemo)
- [Reference](#reference)

notes:

1. *Date: YY/MM, All projects are sorted by first proposed date*
2. *Dec=Decoder; Enc=Encoder; CTX=Context Length; WD=Window Length*
3. *We provide Huggingface (HF) checkpoint by default*

## Pretrained Model

| Model                        | Available Size                                                                                                                                                                                                                                                                                                                                                                                                                               | CTX   | Architecture | Training Data                                                                                                                                                                                     | Link                                                                                                                                                                                                      | Date  | Affiliation  |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ |
| Yi                           | [6B](https://huggingface.co/01-ai/Yi-6B)/[34B](https://huggingface.co/01-ai/Yi-34B)                                                                                                                                                                                                                                                                                                                                                                | 4K WD | Dec          | ?                                                                                                                                                                                                 | [code](https://github.com/01-ai/Yi)                                                                                                                                                                          | 11/23 | 01AI         |
| Skywork                      | [7B](https://huggingface.co/Skywork/Skywork-13B-Base)/[13B](https://huggingface.co/datasets/Skywork/SkyPile-150B)                                                                                                                                                                                                                                                                                                                                  | 4K    | Dec          | self-construct (500B/2TB/3.1TB tokens, Chinese&English)                                                                                                                                          | [code](https://github.com/SkyworkAI/Skywork)                                                                                                                                                                 | 10/23 | SkyworkAI    |
| Mistral                      | [7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)                                                                                                                                                                                                                                                                                                                                                                                          | 4K WD | Dec          | ?                                                                                                                                                                                                 | [code](https://github.com/mistralai/mistral-src), [blog](https://mistral.ai/news/announcing-mistral-7b/)                                                                                                        | 09/23 | Mistral AI   |
| XVERSE                       | [13B](https://huggingface.co/xverse/XVERSE-13B)                                                                                                                                                                                                                                                                                                                                                                                                 | 8K    | Dec          | self-construct (1.4T tokens, Multilingual)                                                                                                                                                        | [code](https://github.com/xverse-ai/XVERSE-13B)                                                                                                                                                              | 08/23 | xverse.ai    |
| Qwen-7B                      | [7B](https://huggingface.co/Qwen/Qwen-7B)                                                                                                                                                                                                                                                                                                                                                                                                       | 2K    | Dec          | self-construct (2.2T tokens)                                                                                                                                                                      | [code](https://github.com/QwenLM/Qwen-7B)                                                                                                                                                                    | 08/23 | Alibaba      |
| Llama 2 `Commercial`       | [7B](https://huggingface.co/meta-llama/Llama-2-7b), [13B](https://huggingface.co/meta-llama/Llama-2-13b), [70B](https://huggingface.co/meta-llama/Llama-2-70b)                                                                                                                                                                                                                                                                                        | 4K    | Dec          | self-construct (2T tokens)                                                                                                                                                                        | [code](https://github.com/facebookresearch/llama), [blog](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)                                                        | 07/23 | Meta         |
| PolyLM                       | [1.7B](https://huggingface.co/DAMO-NLP-MT/polylm-1.7b), [13B](https://huggingface.co/DAMO-NLP-MT/polylm-7b)                                                                                                                                                                                                                                                                                                                                        | 2K    | Dec          | self-construct (PT + INST, Multilingual)                                                                                                                                                          | [blog](https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation/summary)                                                                                                                             | 07/23 | Alibaba      |
| InternLM                     | [7B](https://huggingface.co/internlm/internlm-7b),[Original](https://github.com/InternLM/InternLM#model-zoo)                                                                                                                                                                                                                                                                                                                                       | 8K    | Dec          | self-construct (1.6T)                                                                                                                                                                             | [code](https://github.com/InternLM/InternLM)                                                                                                                                                                 | 07/23 | SenseTime    |
| 天鹰 (Aquila)                | 7B/33B,[Original](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)                                                                                                                                                                                                                                                                                                                                                            | 2K    | Dec          | ? (English & Chinese)                                                                                                                                                                             | [code](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)                                                                                                                                    | 06/23 | BAAI         |
| XGen                         | [7B-4K](https://huggingface.co/Salesforce/xgen-7b-4k-base)/[7B-8K]()                                                                                                                                                                                                                                                                                                                                                                               | 4K/8K | Dec          | [manual-mixture](https://blog.salesforceairesearch.com/xgen/) (1.2T/1.5T tokens, Multilingual)                                                                                                      | [code](https://github.com/salesforce/xgen), [blog](https://blog.salesforceairesearch.com/xgen/)                                                                                                                | 06/23 | Salesforce   |
| baichuan<br />`Commercial` | [7B](https://huggingface.co/baichuan-inc/baichuan-7B)                                                                                                                                                                                                                                                                                                                                                                                           | 4K    | Dec          | self-construct (1.2T tokens, English&Chinese)                                                                                                                                                     | [code](https://github.com/baichuan-inc/baichuan-7B)                                                                                                                                                          | 06/23 | Baichuan     |
| Tigerbot                     | [7B](https://huggingface.co/TigerResearch/tigerbot-7b-base)                                                                                                                                                                                                                                                                                                                                                                                     | 2K    | Dec          | [self-construct](https://github.com/TigerResearch/TigerBot#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) (100GB,  web, book, English&Chinese)                                                     | [code](https://github.com/TigerResearch/TigerBot)                                                                                                                                                            | 06/23 | Tigerobo     |
| Falcon                       | [7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b)                                                                                                                                                                                                                                                                                                                                                      | 2K    | Dec          | [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) (600B tokens, English, web)                                                                                                  | [blog](https://falconllm.tii.ae/)                                                                                                                                                                            | 05/23 | TII          |
| StarCoder `code`           | [15.5B](https://huggingface.co/bigcode/starcoderbase)                                                                                                                                                                                                                                                                                                                                                                                           | 8K    | Dec          | [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup) (1TB code)                                                                                                                     | [paper](https://arxiv.org/abs/2305.06161), [code](https://github.com/bigcode-project/starcoder/tree/main)                                                                                                       | 05/23 | Huggingface  |
| CodeGen2 `code`            | [1B](https://huggingface.co/Salesforce/codegen2-1B)/[3.7B](https://huggingface.co/Salesforce/codegen2-3_7B)/[7B](https://huggingface.co/Salesforce/codegen2-7B)/[16B](https://huggingface.co/Salesforce/codegen2-16B)                                                                                                                                                                                                                                    | 2K    | Dec          | [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup) (3TB code)                                                                                                                     | [paper](https://arxiv.org/abs/2305.02309), [code](https://github.com/salesforce/CodeGen), [blog](https://huggingface.co/blog/starcoder)                                                                            | 05/23 | Salesforce   |
| StableLM-Alpha               | [3B](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)/[7B](https://huggingface.co/stabilityai/stablelm-base-alpha-7b),[Original](https://github.com/Stability-AI/StableLM#stablelm-alpha)                                                                                                                                                                                                                                                   | 4K    | Dec          | self-construct (1.2T tokens, English)                                                                                                                                                             | [code](https://github.com/Stability-AI/StableLM#stablelm-alpha)                                                                                                                                              | 04/23 | Stability.AI |
| Pythia                       | [1B](https://huggingface.co/EleutherAI/pythia-1b)/[1.4B](https://huggingface.co/EleutherAI/pythia-1.4b)/[2.8B](https://huggingface.co/EleutherAI/pythia-2.8b)/[6.9B](https://huggingface.co/EleutherAI/pythia-6.9b)/[12B](https://huggingface.co/EleutherAI/pythia-12b),[Original](https://github.com/EleutherAI/pythia#models)                                                                                                                                | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2304.01373), [code](https://github.com/EleutherAI/pythia), [blog](https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling) | 04/23 | EleutherAI   |
| MPT                          | [7B](https://huggingface.co/mosaicml/mpt-7b)/[30B](https://huggingface.co/mosaicml/mpt-30b)                                                                                                                                                                                                                                                                                                                                                        | 8K    | Dec          | [C4](https://huggingface.co/datasets/allenai/c4), [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup), [MC4](https://huggingface.co/datasets/mc4) (1T tokens, text + code, Multilingual) | [code](https://github.com/mosaicml/llm-foundry#mpt), [blog](https://www.mosaicml.com/blog/mpt-7b)                                                                                                               | 04/23 | MosaicML     |
| Cerebras-GPT                 | [1.3B](https://huggingface.co/cerebras/Cerebras-GPT-1.3B)/[2.7B](https://huggingface.co/cerebras/Cerebras-GPT-2.7B)/[6.7B](https://huggingface.co/cerebras/Cerebras-GPT-6.7B)/[13B](https://huggingface.co/cerebras/Cerebras-GPT-13B)                                                                                                                                                                                                                    | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2304.03208),[code](https://github.com/Cerebras/modelzoo), [blog](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)               | 03/23 | Cerebras     |
| LLaMA                        | [7B](https://huggingface.co/huggyllama/llama-7b)/[13B](https://huggingface.co/huggyllama/llama-13b)/[33B](https://huggingface.co/huggyllama/llama-30b)/[65B](https://huggingface.co/huggyllama/llama-65b), [Request](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform) or [OpenLLaMA](https://github.com/openlm-research/open_llama)                                                                       | 2K    | Dec          | manual-mixture (1.4T tokens) or[RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)                                                                                                | [paper](https://arxiv.org/abs/2302.13971), [code](https://github.com/facebookresearch/llama), [blog](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)                                             | 02/23 | Meta         |
| Galactica                    | [1.3B](https://huggingface.co/facebook/galactica-1.3b)/[6.7B](https://huggingface.co/facebook/galactica-6.7b)/[30B](https://huggingface.co/facebook/galactica-30b)/[120B](https://huggingface.co/facebook/galactica-120b)                                                                                                                                                                                                                                | 2K    | Dec          | scientific text and data (106B tokens)                                                                                                                                                            | [paper](https://arxiv.org/abs/2211.09085), [blog](https://galactica.org/)                                                                                                                                       | 11/22 | Meta         |
| BLOOM                        | [1.1B](https://huggingface.co/bigscience/bloom-1b1)/[1.7B](https://huggingface.co/bigscience/bloom-1b1)/[3B](https://huggingface.co/bigscience/bloom-3b)/[7.1B](https://huggingface.co/bigscience/bloom-7b1)                                                                                                                                                                                                                                             | 2K    | Dec          | [ROOTS](https://huggingface.co/spaces/bigscience-data/roots-search) (388B tokens, Multilingual, HF datasets)                                                                                        | [paper](https://arxiv.org/abs/2211.05100)                                                                                                                                                                    | 11/22 | BigScience   |
| RWKV-v4                      | [1.4B](https://huggingface.co/BlinkDL/rwkv-4-pile-1b5)/[3B](https://huggingface.co/BlinkDL/rwkv-4-pile-3b)/[7B](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)/[14B](https://huggingface.co/BlinkDL/rwkv-4-pile-14b)                                                                                                                                                                                                                                     | -     | RWKV         | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2305.13048), [code](https://github.com/BlinkDL/RWKV-LM)                                                                                                                           | 09/22 | BlinkDL      |
| YALM                         | [100B](https://huggingface.co/yandex/yalm-100b/tree/main)                                                                                                                                                                                                                                                                                                                                                                                       | 1K    | Dec          | [The Pile](https://pile.eleuther.ai/), [Texts in Russian]() (300B tokens, English + Russian)                                                                                                           | [code](https://github.com/yandex/YaLM-100B), [blog](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6)                                   | 06/22 | Yandex       |
| UL2                          | [20B](https://huggingface.co/google/ul2#:~:text=UL2%20is%20a%20unified%20framework%20for%20pretraining%20models,downstream%20fine-tuning%20is%20associated%20with%20specific%20pre-training%20schemes.)                                                                                                                                                                                                                                         | 0.5K  | Enc-Dec/Dec  | [C4](https://www.tensorflow.org/datasets/catalog/c4) (365B tokens, English, web)                                                                                                                    | [paper](https://arxiv.org/abs/2205.05131v1),[blog](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html)                                                                                 | 05/22 | Google       |
| OPT                          | [1.3B](https://huggingface.co/facebook/opt-1.3b)/[2.7B](https://huggingface.co/facebook/opt-2.7b)/[6.7B](https://huggingface.co/facebook/opt-6.7b)/[13B](https://huggingface.co/facebook/opt-13b)/[30B](https://huggingface.co/facebook/opt-30b)/[66B](https://huggingface.co/facebook/opt-66b)/[175B (request)](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT), [Original](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT) | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/), [PushShift.io Reddit](https://zenodo.org/record/3608135), [Roberta](https://github.com/facebookresearch/fairseq/issues/2947) (180B tokens, mainly English)         | [paper](https://arxiv.org/abs/2205.01068), [code](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT)                                                                                          | 05/22 | Meta         |
| GPT-NeoX                     | [20B-Original](https://github.com/EleutherAI/gpt-neox#gpt-neox-20b)                                                                                                                                                                                                                                                                                                                                                                             | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [code](https://github.com/EleutherAI/gpt-neox), [paper](https://arxiv.org/abs/2204.06745)                                                                                                                       | 04/22 | EleutherAI   |
| GPT-J                        | [6B](https://huggingface.co/EleutherAI/gpt-j-6b), [Original](https://github.com/kingoflolz/mesh-transformer-jax/#links)                                                                                                                                                                                                                                                                                                                           | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [code](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b)                                                                                                                                         | 05/21 | EleutherAI   |
| GPT-Neo                      | [1.3B](https://huggingface.co/EleutherAI/gpt-neo-1.3B)/[2.7B](https://huggingface.co/EleutherAI/gpt-neo-2.7B), [Original](https://github.com/EleutherAI/gpt-neo#pretrained-models)                                                                                                                                                                                                                                                                    | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/) (400B tokens, English)                                                                                                                                        | [code](https://www.eleuther.ai/artifacts/gpt-neo), [blog](https://www.eleuther.ai/artifacts/gpt-neo)                                                                                                           | 03/21 | EleutherAI   |
| GLM                          | [2B](https://huggingface.co/THUDM/glm-2b)/[10B](https://huggingface.co/THUDM/glm-10b)/[10B-CHN](https://huggingface.co/THUDM/glm-10b-chinese), [Original](https://github.com/THUDM/GLM#pretrained-models)                                                                                                                                                                                                                                                | 1K    | Enc-Dec      | [The Pile](https://pile.eleuther.ai/) (400B tokens, English) or [WuDaoCorpora](https://data.baai.ac.cn/details/WuDaoCorporaText) (1080B tokens, Chinese)                                             | [paper](https://arxiv.org/abs/2103.10360), [code](https://github.com/THUDM/GLM)                                                                                                                                | 03/21 | THU          |
| mT5                          | [1.2B](https://huggingface.co/google/mt5-large)/[3.7B](https://huggingface.co/google/mt5-xl)/[13B](https://huggingface.co/google/mt5-xxl),[Original](https://github.com/google-research/multilingual-t5#released-model-checkpoints)                                                                                                                                                                                                                      | 0.5K  | Enc-Dec      | [mC4](https://www.tensorflow.org/datasets/catalog/c4#c4multilingual) (1T tokens, Multilingual, web)                                                                                                 | [paper](https://aclanthology.org/2021.naacl-main.41/), [code](https://github.com/google-research/multilingual-t5)                                                                                               | 10/20 | Google       |
| T5                           | [3B](https://huggingface.co/t5-3b)/[11B](https://huggingface.co/t5-11b), [Original](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints)                                                                                                                                                                                                                                                                  | 0.5K  | Enc-Dec      | [C4](https://www.tensorflow.org/datasets/catalog/c4) (365B tokens, English, web)                                                                                                                   | [paper](https://jmlr.org/papers/v21/20-074.html), [code](https://github.com/google-research/text-to-text-transfer-transformer)                                                                                  | 10/19 | Google       |

## Multitask Supervised Finetuned Model

| Model     | Available Size                                                                                                                                                                                       | CTX  | Architecture | Base Model | Tuning Data                                                                             | Link                                                                                       | Date  | Affiliation |
| --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---- | ------------ | ---------- | --------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ----- | ----------- |
| Bloomz    | [1.1B](https://huggingface.co/bigscience/bloomz-1b1)/[1.7B](https://huggingface.co/bigscience/bloomz-1b1)/[3B](https://huggingface.co/bigscience/bloomz-3b)/[7.1B](https://huggingface.co/bigscience/bloomz-7b1) | 2K   | Dec          | Bloom      | [xP3](https://huggingface.co/datasets/bigscience/xP3) (Multitask, Multilingual)           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| Bloomz-mt | [7.1B](https://huggingface.co/bigscience/bloomz-7b1-mt)                                                                                                                                                 | 2K   | Dec          | Bloom      | [xP3mt](https://huggingface.co/datasets/bigscience/xP3mt) (Multitask, Multilingual+nonEN) | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| Bloomz-p3 | [7.1B](https://huggingface.co/bigscience/bloomz-7b1-p3)                                                                                                                                                 | 2K   | Dec          | Bloom      | [P3](https://huggingface.co/datasets/bigscience/P3) (Multitask)                           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| mT0       | [1.2B](https://huggingface.co/bigscience/mt0-large)/[3.7B](https://huggingface.co/bigscience/mt0-xl)/[13B](https://huggingface.co/bigscience/mt0-xll)                                                         | 0.5K | Enc-Dec      | mT5        | [xP3mt](https://huggingface.co/datasets/bigscience/xP3mt) (Multitask, Multilingual+nonEN) | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| mT0-mt    | [13B](https://huggingface.co/bigscience/mt0-xxl-mt)                                                                                                                                                     | 0.5K | Enc-Dec      | mT5        | [xP3](https://huggingface.co/datasets/bigscience/xP3) (Multitask, Multilingual)           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| mT0-p3    | [13B](https://huggingface.co/bigscience/mt0-xll-p3)                                                                                                                                                     | 0.5K | Enc-Dec      | mT5        | [P3](https://huggingface.co/datasets/bigscience/P3) (Multitask)                           | [paper](https://arxiv.org/abs/2211.01786), [code](https://github.com/bigscience-workshop/xmtf)  | 11/22 | Bigscience  |
| T0        | [3B](https://huggingface.co/bigscience/T0_3B)/[11B](https://huggingface.co/bigscience/T0pp)                                                                                                                | 0.5K | Enc-Dec      | T5         | [P3](https://huggingface.co/datasets/bigscience/P3) (Multitask)                           | [paper](https://arxiv.org/abs/2110.08207), [code](https://github.com/bigscience-workshop/t-zero) | 10/21 | Bigscience  |

## Instruction Finetuned Model

### English

| Model                   | Available Size                                                                                                                                                       | CTX   | Architecture | Base Model | Tuning Data                                                                                                                    | Link                                                                                                                                                     | Date  | Affiliation   |
| ----------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ | ---------- | ------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------- |
| WizardMath `math`     | [7B](https://huggingface.co/WizardLM/WizardMath-7B-V1.0)/[13B](https://huggingface.co/WizardLM/WizardMath-13B-V1.0)/[70B](https://huggingface.co/WizardLM/WizardMath-7B-V1.0) | 4K    | Dec          | LLaMA2     | self-construct (math)                                                                                                          | [paper](https://arxiv.org/abs/2308.09583),[code](https://github.com/nlpxucan/WizardLM/tree/main/WizardMath)                                                    | 08/23 | MSRA          |
| OpenChat                | [13B-2K](https://huggingface.co/openchat/openchat)/[13B-8K](https://huggingface.co/openchat/openchat_8192)                                                                 | 2K/8K | Dec          | LLaMA      | [filterd](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset) (6K from ShareGPT)                                 | [code](https://github.com/imoneoi/openchat)                                                                                                                 | 07/23 | personal      |
| LongChat                | [7B](https://huggingface.co/lmsys/longchat-7b-16k)/[13B](https://huggingface.co/lmsys/longchat-13b-16k)                                                                    | 16K   | Dec          | LLaMA      | [self-construct](https://lmsys.org/blog/2023-06-29-longchat/#step-2-finetuning-on-curated-conversation-data) (FastChat pipeline) | [code](https://github.com/DachengLi1/LongChat), [blog](https://lmsys.org/blog/2023-06-29-longchat)                                                             | 06/23 | LMSYS         |
| Tulu                    | [65B](https://huggingface.co/allenai/tulu-65b)                                                                                                                          | 2K    | Dec          | LLaMA      | Tulu mix (Human+GPT mixture)                                                                                                   | [paper](https://arxiv.org/abs/2306.04751), [code](https://github.com/allenai/open-instruct)                                                                   | 06/23 | AI2           |
| WizardCoder `code`    | [15B](https://huggingface.co/WizardLM/WizardCoder-15B-V1.0)                                                                                                             | 8K    | Dec          | StarCoder  | (code)                                                                                                                         | [paper](https://arxiv.org/abs/2306.08568), [code](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder)                                                  | 06/23 | MSRA          |
| WizardLM                | [7B](https://huggingface.co/WizardLM/WizardLM-7B-V1.0)/[13B](https://huggingface.co/WizardLM/WizardLM-13B-V1.0)/[30B](https://huggingface.co/WizardLM/WizardLM-30B-V1.0)      | 2K    | Dec          | LLaMA      | [Evol-Instruct](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k) (143K instruction)                      | [paper](https://arxiv.org/abs/2304.12244), [code](https://github.com/nlpxucan/WizardLM)                                                                        | 06/23 | MSRA          |
| Lion                    | [7B](https://huggingface.co/YuxinJiang/Lion)                                                                                                                            | 2K    | Dec          | LLaMA      | distill (70K instruction)                                                                                                      | [paper](https://arxiv.org/abs/2305.12870), [code](https://github.com/YJiangcm/Lion)                                                                            | 05/23 | HKUST         |
| UltraChat               | [13B](https://huggingface.co/openbmb/UltraLM-13b)                                                                                                                       | 2K    | Dec          | LLamA      | [ultrachat](https://huggingface.co/datasets/stingning/ultrachat) (1.4M dialogues)                                                | [paper](https://arxiv.org/abs/2305.14233), [code](https://github.com/thunlp/UltraChat)                                                                         | 05/23 | THU           |
| Dromedary               | [65B](https://huggingface.co/zhiqings/dromedary-65b-lora-delta-v0)                                                                                                      | 2K    | Dec          | LLaMA      | [self-aligned](https://huggingface.co/datasets/zhiqings/dromedary-65b-verbose-clone-v0) (360K instruction)                       | [paper](https://arxiv.org/abs/2305.03047), [code](https://github.com/IBM/Dromedary)                                                                           | 05/23 | IBM           |
| Dolly-v2                | [12B](https://huggingface.co/databricks/dolly-v2-12b)                                                                                                                   | 2K    | Dec          | Pythia     | [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k) (15k instruction/response)               | [code](h