# Open Source LLMs

Table of Contents

- [Pretrained Model](#pretrained-model)
- [Multitask Supervised Finetuned Model](#multitask-supervised-finetuned-model)
- [Instruction Finetuned Model](#instruction-finetuned-model)
  - [English](#english)
  - [Chinese](#chinese)
  - [Multilingual](#multilingual)
- [Human Feedback Finetuned Model](#human-feedback-finetuned-models)
- [Domain Finetuned Model](#domain-finetuned-model)
- [Open Source Projects](#open-source-projects)
  - [reproduce/framework](#reproduceframework)
  - [accelerate](#accelerate)
  - [evaluation](#evaluation)
  - [deployment/demo](#deploymentdemo)
- [Reference](#reference)

notes:

1. *Date: YY/MM, All projects are sorted by first proposed date*
2. *Dec=Decoder; Enc=Encoder; CTX=Context Length; WD=Window Length*
3. *We provide Huggingface (HF) checkpoint by default*

## Pretrained Model

| Model                        | Available Size                                                                                                                                                                                                                                                                                                                                                                                                                               | CTX   | Architecture | Training Data                                                                                                                                                                                     | Link                                                                                                                                                                                                      | Date  | Affiliation  |
| ---------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----- | ------------ |
| Yi                           | [6B](https://huggingface.co/01-ai/Yi-6B)/[34B](https://huggingface.co/01-ai/Yi-34B)                                                                                                                                                                                                                                                                                                                                                                | 4K WD | Dec          | ?                                                                                                                                                                                                 | [code](https://github.com/01-ai/Yi)                                                                                                                                                                          | 11/23 | 01AI         |
| Skywork                      | [7B](https://huggingface.co/Skywork/Skywork-13B-Base)/[13B](https://huggingface.co/datasets/Skywork/SkyPile-150B)                                                                                                                                                                                                                                                                                                                                  | 4K    | Dec          | self-construct (500B/2TB/3.1TBÂ tokens, Chinese&English)                                                                                                                                          | [code](https://github.com/SkyworkAI/Skywork)                                                                                                                                                                 | 10/23 | SkyworkAI    |
| Mistral                      | [7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)                                                                                                                                                                                                                                                                                                                                                                                          | 4K WD | Dec          | ?                                                                                                                                                                                                 | [code](https://github.com/mistralai/mistral-src), [blog](https://mistral.ai/news/announcing-mistral-7b/)                                                                                                        | 09/23 | Mistral AI   |
| XVERSE                       | [13B](https://huggingface.co/xverse/XVERSE-13B)                                                                                                                                                                                                                                                                                                                                                                                                 | 8K    | Dec          | self-construct (1.4T tokens, Multilingual)                                                                                                                                                        | [code](https://github.com/xverse-ai/XVERSE-13B)                                                                                                                                                              | 08/23 | xverse.ai    |
| Qwen-7B                      | [7B](https://huggingface.co/Qwen/Qwen-7B)                                                                                                                                                                                                                                                                                                                                                                                                       | 2K    | Dec          | self-construct (2.2T tokens)                                                                                                                                                                      | [code](https://github.com/QwenLM/Qwen-7B)                                                                                                                                                                    | 08/23 | Alibaba      |
| Llama 2 `Commercial`       | [7B](https://huggingface.co/meta-llama/Llama-2-7b), [13B](https://huggingface.co/meta-llama/Llama-2-13b), [70B](https://huggingface.co/meta-llama/Llama-2-70b)                                                                                                                                                                                                                                                                                        | 4K    | Dec          | self-construct (2T tokens)                                                                                                                                                                        | [code](https://github.com/facebookresearch/llama), [blog](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)                                                        | 07/23 | Meta         |
| PolyLM                       | [1.7B](https://huggingface.co/DAMO-NLP-MT/polylm-1.7b), [13B](https://huggingface.co/DAMO-NLP-MT/polylm-7b)                                                                                                                                                                                                                                                                                                                                        | 2K    | Dec          | self-construct (PT + INST, Multilingual)                                                                                                                                                          | [blog](https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation/summary)                                                                                                                             | 07/23 | Alibaba      |
| InternLM                     | [7B](https://huggingface.co/internlm/internlm-7b),[Original](https://github.com/InternLM/InternLM#model-zoo)                                                                                                                                                                                                                                                                                                                                       | 8K    | Dec          | self-construct (1.6T)                                                                                                                                                                             | [code](https://github.com/InternLM/InternLM)                                                                                                                                                                 | 07/23 | SenseTime    |
| å¤©é¹° (Aquila)                | 7B/33B,[Original](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)                                                                                                                                                                                                                                                                                                                                                            | 2K    | Dec          | ? (English & Chinese)                                                                                                                                                                             | [code](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)                                                                                                                                    | 06/23 | BAAI         |
| XGen                         | [7B-4K](https://huggingface.co/Salesforce/xgen-7b-4k-base)/[7B-8K]()                                                                                                                                                                                                                                                                                                                                                                               | 4K/8K | Dec          | [manual-mixture](https://blog.salesforceairesearch.com/xgen/)Â (1.2T/1.5T tokens, Multilingual)                                                                                                      | [code](https://github.com/salesforce/xgen),Â [blog](https://blog.salesforceairesearch.com/xgen/)                                                                                                                | 06/23 | Salesforce   |
| baichuan<br />`Commercial` | [7B](https://huggingface.co/baichuan-inc/baichuan-7B)                                                                                                                                                                                                                                                                                                                                                                                           | 4K    | Dec          | self-construct (1.2T tokens, English&Chinese)                                                                                                                                                     | [code](https://github.com/baichuan-inc/baichuan-7B)                                                                                                                                                          | 06/23 | Baichuan     |
| Tigerbot                     | [7B](https://huggingface.co/TigerResearch/tigerbot-7b-base)                                                                                                                                                                                                                                                                                                                                                                                     | 2K    | Dec          | [self-construct](https://github.com/TigerResearch/TigerBot#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE) (100GB,Â Â web, book, English&Chinese)                                                     | [code](https://github.com/TigerResearch/TigerBot)                                                                                                                                                            | 06/23 | Tigerobo     |
| Falcon                       | [7B](https://huggingface.co/tiiuae/falcon-7b)/[40B](https://huggingface.co/tiiuae/falcon-40b)                                                                                                                                                                                                                                                                                                                                                      | 2K    | Dec          | [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)Â (600B tokens, English, web)                                                                                                  | [blog](https://falconllm.tii.ae/)                                                                                                                                                                            | 05/23 | TII          |
| StarCoder `code`           | [15.5B](https://huggingface.co/bigcode/starcoderbase)                                                                                                                                                                                                                                                                                                                                                                                           | 8K    | Dec          | [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup)Â (1TB code)                                                                                                                     | [paper](https://arxiv.org/abs/2305.06161), [code](https://github.com/bigcode-project/starcoder/tree/main)                                                                                                       | 05/23 | Huggingface  |
| CodeGen2 `code`            | [1B](https://huggingface.co/Salesforce/codegen2-1B)/[3.7B](https://huggingface.co/Salesforce/codegen2-3_7B)/[7B](https://huggingface.co/Salesforce/codegen2-7B)/[16B](https://huggingface.co/Salesforce/codegen2-16B)                                                                                                                                                                                                                                    | 2K    | Dec          | [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup)Â (3TB code)                                                                                                                     | [paper](https://arxiv.org/abs/2305.02309), [code](https://github.com/salesforce/CodeGen), [blog](https://huggingface.co/blog/starcoder)                                                                            | 05/23 | Salesforce   |
| StableLM-Alpha               | [3B](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)/[7B](https://huggingface.co/stabilityai/stablelm-base-alpha-7b),[Original](https://github.com/Stability-AI/StableLM#stablelm-alpha)                                                                                                                                                                                                                                                   | 4K    | Dec          | self-construct (1.2T tokens, English)                                                                                                                                                             | [code](https://github.com/Stability-AI/StableLM#stablelm-alpha)                                                                                                                                              | 04/23 | Stability.AI |
| Pythia                       | [1B](https://huggingface.co/EleutherAI/pythia-1b)/[1.4B](https://huggingface.co/EleutherAI/pythia-1.4b)/[2.8B](https://huggingface.co/EleutherAI/pythia-2.8b)/[6.9B](https://huggingface.co/EleutherAI/pythia-6.9b)/[12B](https://huggingface.co/EleutherAI/pythia-12b),[Original](https://github.com/EleutherAI/pythia#models)                                                                                                                                | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/)Â (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2304.01373), [code](https://github.com/EleutherAI/pythia), [blog](https://www.eleuther.ai/papers-blog/pythia-a-suite-for-analyzing-large-language-modelsacross-training-and-scaling) | 04/23 | EleutherAI   |
| MPT                          | [7B](https://huggingface.co/mosaicml/mpt-7b)/[30B](https://huggingface.co/mosaicml/mpt-30b)                                                                                                                                                                                                                                                                                                                                                        | 8K    | Dec          | [C4](https://huggingface.co/datasets/allenai/c4), [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup), [MC4](https://huggingface.co/datasets/mc4)Â (1T tokens, text + code, Multilingual) | [code](https://github.com/mosaicml/llm-foundry#mpt), [blog](https://www.mosaicml.com/blog/mpt-7b)                                                                                                               | 04/23 | MosaicML     |
| Cerebras-GPT                 | [1.3B](https://huggingface.co/cerebras/Cerebras-GPT-1.3B)/[2.7B](https://huggingface.co/cerebras/Cerebras-GPT-2.7B)/[6.7B](https://huggingface.co/cerebras/Cerebras-GPT-6.7B)/[13B](https://huggingface.co/cerebras/Cerebras-GPT-13B)                                                                                                                                                                                                                    | 2K    | Dec          | [The Pile](https://pile.eleuther.ai/)Â (400B tokens, English)                                                                                                                                        | [paper](https://arxiv.org/abs/2304.03208),[code](https://github.com/Cerebras/modelzoo), [blog](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)               | 03/23 | Cerebras     |
| LLaMA                        | [7B](https://huggingface.co/huggyllama/llama-7b)/[13B](https://huggingface.co/huggyllama/llama-13b)/[33B](https://huggingface.co/huggyllama/llama-30b)/[65B](https://huggingface.co/huggyllama/llama-65b), [Request](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform)Â or [OpenLLaMA](https://github.com/openlm-research/open_llama)                                                                       | 2K    | Dec          | manual-mixtureÂ (1.4TÂ tokens) or[RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data)                                                                                                | [paper](https://arxiv.org/abs/2302.13971), [code](https://github.com/facebookresearch/llama), [blog](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)                                             | 02/23 | Meta         |
| Galactica                    | [1.3B](https://huggingface.co/facebook/galactica-1.3b)/[6.7B](https://huggingface.co/facebook/galactica-6.7b)/[30B](https://huggingface.co/facebook/galactica-30b)/[120B](https://huggingface.co/facebook/galactica-120b)                                                                                                                                                                                                                                | 2K    | Dec          | scientific text and data (106B tokens)                                                                                                                                                            | [paper](https://arxiv.org/abs/2211.09085), [blog](https://galactica.org/)                                                                                                                                       | 11/22 | Meta         |
| BLOOM                        | [1.1B](https://huggingface.co/bigscience/bloom-1b1)/[1.7B](https://huggingface.co/bigscience/bloom-1b1)/[3B](https://huggingface.co/bigscience/bloom-3b)/[7.1B](https://huggingface.co/bigscience/bloom-7b1)                                                                                                                                                                                                                                             | 2K    | Dec          | [ROOTS](https://huggingface.co/spaces/bigscience-data/roots-search) (388B tokens, Multilingual,Â HF datasets)                                                                                        | [paper](https://arxiv.org/abs/2211.05100)                                                                                                                                                                    | 11/22 | BigScience   |
| RWKV-v4                      | [1.4B](https://huggingface.co/BlinkDL/rwkv-4-pile-1b5)/[3B](https://huggingface.co/BlinkDL/rwkv-4-pile-3b)/[7B](https://huggingface.co/BlinkDL/rwkv-4-pile-7b)/[14B